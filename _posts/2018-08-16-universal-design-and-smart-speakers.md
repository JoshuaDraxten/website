---
layout: post
title:  "Universal Design and Smart Speakers"
date:   2018-08-16
categories: thoughts
written_at: Brooklyn, NY
---

Technology designed for people with disabilities often benefits everyone in society. Sidewalk ramps, audiobooks, automatic doors and drinking straws are all technology that were built for people with disabilities but are convenient things everyone in society uses on a day-to-day basis. Smart speakers are a useful tool for deaf users, but I fear that their current design benefits them by incident and not on purpose. This needs to change. A smart speaker will never fulfill the dream of conversational computing until it is reliable tool for users who cannot see.

I would like to make clear that I am not implying that computational tasks are inherently visual and people who are blind are incapable of completing them. I believe that most if not all hurdles that are applied to people who are blind when interfacing with tech are through bad design.

The issue is obvious from the beginning of the on boarding process of any smart speaker. The first thing someone needs to do when they get their Google Home is to download an app to their phone and go through endless settings. If you’ve never done so, I recommend turning on VoiceOver on your phone, closing your eyes and try to install a new app with an unfamiliar UI and interface with it. Even if you get very good at voiceover, there is a definitely more friction involved in operating new, unfamiliar apps without being able to see them.

As far as I can tell the only important setting that must be typed instead of said in the on boarding process is the WiFi password, everything else is extra. There needs to be a new way to input passwords that operates using voice input, because downloading an app and spending 2 minutes going through a basic settings screen is too much friction. It’s too much friction for someone who cannot see, and it’s too much friction for me.

For another example, imagine if you’re craving some sugar cookies and you go to your kitchen and ask your smart speaker “Hey supercomputer what are some sugar cookie recipes”. From what I can see, every major smart speaker will defer you to your phone with a list of recipes for you to sort through. For the Google Assistant, once you’ve chosen the recipe, you can have it walk you through the cooking steps.

The assumption is that any list based content is better viewed than heard, which is true if an audio system just lists out the options one by one. But smart speakers have access to thousands of reviews, can parse through ingredients, do on the fly unsupervised classification to ask followup questions to help someone come to a recipe choice in a frictionless audio-only way. This would not just be better for people who cannot see, this would be better for me.

Every case where a smart speaker sends someone to their phone is a failure of design. We currently think some tasks require using a screen to accomplish because there has not been enough effort in solving universal design problems of audio interfaces. Solving these problems will make better tools for all end users just as countless accessibility solutions have in the past.
